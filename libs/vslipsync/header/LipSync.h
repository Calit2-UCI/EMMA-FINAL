
#ifndef __LipSync_h__
#define __LipSync_h__

// The following ifdef block is the standard way of creating macros which make exporting 
// from a DLL simpler. All files within this DLL are compiled with the LIPSYNC_EXPORTS
// symbol defined on the command line. this symbol should not be defined on any project
// that uses this DLL. This way any other project whose source files include this file see 
// VISAGE_DECLSPEC functions as being imported from a DLL, wheras this DLL sees symbols
// defined with this macro as being exported.

#define LS_MODE_REAL_TIME 0
#define LS_MODE_OFFLINE	1

#define LS_HE_MODE_VISEME_BASED 10
#define LS_HE_MODE_FRAME_BASED 11

#include "FbaAction.h"
#ifndef MSVC
#include <pthread.h>
#endif

#include <iostream>
#include "AudioStreamLipSync.hpp"

namespace VisageSDK
{

class CVisemes;
class LipSyncEventHandler;
class VisageLipSync;

/**
* Lip Sync event handler.
* This virtual class is used to catch and handle events produced by VisageLipSync. It provides the mechanism
* which allows an application to capture the lip sync data in real time, as it is generated.
*
* An implementation of this class must implement the function handleEvent, which
* will be called by VisageLipSync whenever an event occurs, eg. when the viseme
* is generated by lip sync.
*
* Implemented in visagelipsync.lib
*/
class VISAGE_DECLSPEC LipSyncEventHandler {
public:
	/** Constructor.
	* Initialises the LipSyncEventHandler. 
	*/
	LipSyncEventHandler(void);

	~LipSyncEventHandler(void);

	/** The event handling function. 
	* This is a callback function, called by VisageLipSync when ever an event occur. The exact behavior
	* depends on the mode set by function VisageLipSync::setHandleEventMode() and whether the animation is written in the file. 
	* The following table summarizes the possible modes of operation.
	* 
	* <TABLE>
	* <TR><TD></TD> <TD>VISEME BASED</TD> <TD>FRAME BASED</TD> </TR>
	* <TR><TD>VisageLipsync is not writting the animation into a file</TD> <TD>function is called when new viseme occurs</TD> <TD>function is called once for each audio frame</TD> </TR>
	* <TR><TD>VisageLipsync is writting the animation into a file</TD> <TD>function is called once for each frame of animation</TD><TD>not supported</TD> </TR>
	* </TABLE>
	*
	* - if VisageLipsync is not writting the animation into a file and Handle Event mode is set as viseme based, 
	*   the function is called when a new viseme occurs
	* - if VisageLipsync is not writting the animation into a file and Handle Event mode is set as frame based, 
	*   the function is called once for each audio frame  
	* - if VisageLipsync is writting the animation into a file, function is called once for each frame of animation, 
	*	before the frame is written into a file 
	*
	* This is a virtual function and has to be implemented by each 
	* LipSyncEventHandler type. The handleEvent() function gets the time stamp of the event in 
	* milliseconds since beginning of animation, the current viseme and other data
	* that may be useful for the application.
	* This data is provided by VisageLipSync when it calls the handleEvent() function. 
	* It is intended to be used in any desired way by each implementation of the handleEvent() 
	* function.
	*
	*
	* Note that this function may change the animation parameters contained in FBAPs. This can be used to augment 
	* the animation. Typical use for this would be to implement some head motion, eyeblinks etc.
	* that accompany the lip movements, so the complete animation looks more natural.
	* If VisageLipSync is writting the animation into a file, the modified FBAPs will be written.
	* 
	*
	* @param lipsync current VisageLipSync, the object that calls this handling function 
	* @param pts time of the event in milliseconds, measured from the beginning of the audio signal that is used in VisageLipSync.
	* @param fbaps the FBAPs structure with current interpolated visemes. The interpolated visemes are stored in FBAPs->FAPs->FAP1 
	* and defined with two visemes and blending factor that determines the blending between the two. The visemes and blending factor 
	* are determined by the coarticulation algorithm that ensures the smooth transition between viseme. These interpolated visemes
	* can be directly used for animation for example by mapping viseme onto morph targets 
	* @param cp the CodingParameters used if FBAPs are written into a file by VisageLipSync.
	* @param viseme the last detected viseme. Visemes are numbered as in MPEG-4 standard 
	* (see <a href="../MPEG-4 FBA Overview.pdf">MPEG-4 Face and Body Animation Introduction</a> for the list of visemes)
	* @param wBuf contains one frame of raw audio data (16kHz, 16bit)
	*
	*
	* @see VisageLipSync::setHandleEventMode()
	* @see FBAPs
	* @see VisageLipSync
	* @see CodingParameters
	*/

	virtual void handleEvent(VisageLipSync *lipsync, FBAPs* fbaps, CodingParameters *cp, long pts, int viseme, short* wBuf)=0;

};

/**
* Lip Sync function implementation.
* This class implements the real time lip sync function, i.e. by analysing the speech audio signal it produces
* the corresponding MPEG-4 visemes synchronised to the speech. VisageLipSync is an implementation of FbaAction,
* and therefore it can be played in an FAPlayer (using FAPlayer::addTrack or FAPlayer::playTrack()) just
* as any other FbaAction. This makes it easy to
* achieve real time, on-the-fly facial animation synchronized to audio signal from file or microphone - initialize the FAPlayer, add the VisageLipSync to it, and call the sync() function.
*
* There are two input modes for the audio signal: file or microphone. For file input, PCM format, .wav file, 
* (16 kHz sample rate, 16 bit sample size) is supported. In the microphone input mode, the audio signal
* is captured from the microphone and processed on-the-fly. 
*
* The generated visemes can be written into an MPEG-4 FBA file. Additionally, a callback mechanism
* is available through the LipSyncEventHandler class. This mechanism allows an application
* to capture the lip sync data, visemes, time, audio signal, as it is generated. It can also be used to 
* drive other animation systems or for special purposes such as enhancing the animation. An even simpler way
* to obtain the LipSync data is through the LipSync::getCurrentData() function.
*
* NOTE: The folder "NN" containing data files 0Weight.txt, 1Weight.txt,..., 14Weight.txt must be present in the current folder
* when the lip sync operation is performed.
*
*
* Implemented in visagelipsync.lib
*
* Demonstrated in <a href="../Samples/MS_Visual_C++_6.0/Example7/doc/index.html">Example7</a>.
*/
class VISAGE_DECLSPEC VisageLipSync : public FbaAction 
{
public:
	/** Constructor.
	* Initialises the LipSync. 
	*/
	VisageLipSync(void);
	
	VisageLipSync(bool isSequential);

	~VisageLipSync();

	/** Perform lip sync.
	* This function performs lip synchronisation from file if an input file name is provided.
	* If audioFile is NULL, the lip sync is performed on the audio signal captured from 
	* the microphone.
	*
	* If input audio file is used, the lip sync will stop at the end of the file, and the
	* function will return. However, if microphone input is used, the function returns immediately
	* and the lip synchronisation is performed in a background thread until the 
	* VisageLipSync::stopSync() function is called.
	*
	* If FBAFile is not NULL, the animation resulting from the lip sync is written into this
	* file, encoded as MPEG-4 FBA.
	*
	* If eventHandler is not NULL, the function LipSyncEventHandler::handleEvent() is called when event occurs,
	* such as new viseme generated.
	*
	* The combination of these three parameters makes this function very versatile,
	* so it can be used in many different ways. The most typical ways are:
	* 
	* - produce animation from an audio file and store it into an FBA file; audioFile and FBAFile are used, eventHandler is NULL
	* - produce animation from microphone and store it into an FBA file;  FBAFile is used, eventHandler and audioFile are NULL
	* - produce animation from microphone and render it in real time on the screen;  eventHandler, FBAFile and audioFile are NULL
	* - for additional processing during lip sync, eventHandler may be used to catch the generated visemes and implement specific functions.
	* - use lip sync to drive any animation system other than visage; eventHandler is used to provide viseme timing to the animation system
	*
	*
	* NOTE: The folder "NN" containing data files 0Weight.txt, 1Weight.txt,..., 14Weight.txt must be present in the current folder
	* when the lip sync operation is performed.
	* 
	* @param audioFile input audio file. If NULL, microphone is used for input.
	* @param FBAFile output FBA file for face animation. If NULL, no file is output.
	* @param eventHandler event handling class. If null, events are not handled. If it points to a LipSyncEventHandler object, 
	* the LipSyncEventHandler::handleEvent() function is called during lip sync operation.
	*
	* @see stopSync()
	* @see LipSyncEventHandler
	* @see setHandleEventMode()
	* 
	*  
	*/
	void sync(char* audioFile, char *FBAFile, LipSyncEventHandler *eventHandler, bool mute);



	/** Perform TTS with lip Sync.
	* This function performs lip synchronisation from audio stream
	*
	* If input audio file is used, the lip sync will stop at the end of the file, and the
	* function will return. However, if microphone input is used, the function returns immediately
	* and the lip synchronisation is performed in a background thread until the
	* VisageLipSync::stopSync() function is called.
	*
	* If eventHandler is not NULL, the function LipSyncEventHandler::handleEvent() is called when event occurs,
	* such as new viseme generated.
	*
	*
	* NOTE: The folder "NN" containing data files 0Weight.txt, 1Weight.txt,..., 14Weight.txt must be present in the current folder
	* when the lip sync operation is performed.
	*
	* @param audioFile output audio file. If not NULL, the audio from the TTS is stored in that file
	* @param eventHandler event handling class. If null, events are not handled. If it points to a LipSyncEventHandler object,
	* the LipSyncEventHandler::handleEvent() function is called during lip sync operation.
	*
	* @see stopSync()
	* @see LipSyncEventHandler
	* @see setHandleEventMode()
	*
	*/
	//void VisageLipSync::syncTTS(std::string text, char* audioFile, LipSyncEventHandler *eventHandler, bool mute);



	/** Stop lip sync.
	*
	* This function stops the lip sync. This allows the application to control when the lip synching will stop.
	*
	* @see sync()
	*/
	void stopSync();
	
	bool isFinished();
	

	/* Set lip sync mode.
	* This function sets the lip synchronisation mode to either real time or offline. In real time mode, the analysis of the
	* audio signal is fast enough, and the delay low enough to drive face animation on-the-fly; however, the quality of the
	* analysis is slightly lower in order to achieve such a low delay. In off-line mode, the analysis 
	* algorithm takes into account longer chunks of audio signal, resulting in higher quality of the result, but
	* with a longer delay. This mode is suitable for off line productions where the result of lip sync is written
	* into a file for later reproduction.
	*
	* When microphone is used as input, the synchronisation mode is always real time and this function is
	* ignored.
	*
	* @param mode the mode to set; can be LS_MODE_REAL_TIME or LS_MODE_OFFLINE
	*/
	//void VisageLipSync::setSyncMode(int mode);

	/*
	* Get FBAPs (FbaAction implementation).
	*
	* This function implements the FbaAction interface. Do not call it directly.
	*
	* @param globalTime the time for which the FAPs are requested, in milliseconds
	* @param lastFBAPs the final FBAPs from the previous frame of animation; in this implementation we ignore this parameter
	* @param model the character model currently used in the player; in this implementation we ignore this parameter
	*/
	FBAPs *getFBAPs(long globalTime, FBAPs *lastFBAPs, VisageCharModel *model);


	/*
	* Start (FbaAction implementation).
	*
	* This function implements the FbaAction interface. Do not call it directly.
	*
	* @param globalTime the reference time when the action is started, in milliseconds
	*/
	void start(long globalTime);

	/*
	* Stop playing (FbaAction implementation).
	*
	* This function implements the FbaAction interface. Do not call it directly.
	*
	*/
	void stop();

	/*
	* Return the name of the action type (FbaAction implementation).
	*
	* This function implements the FbaAction interface. Do not call it directly.
	*
	* @return the name of the action type.
	*/
	char* actionTypeName() {return (char *)("VisageLipSync");};

	/** Set handle event mode.
	*
	* This function sets the handle event mode to either viseme based or frame based. In viseme based mode, 
	* the function LipSyncEventHandler::handleEvent() is called when a new viseme occurs. Since the new viseme is calculated 
	* after 4 consecutive audio frames, handleEvent function is called after no less than 4 analyzed frames, 
	* and typically less often than that. In frame based mode, the handleEvent function is called for every analyzed audio frame. 
	* The audio frame is 16 ms long.
	*
	*
	* @param mode the mode to set; can be LS_HE_MODE_VISEME_BASED or LS_HE_MODE_FRAME_BASED
	*/
	void setHandleEventMode(int mode);

	/** Get lip sync data.
	*
	* This function gets the current data. It can be called at any time during lip sync operation. It is assumed that it will be 
	* called from the separate thread. Typical usage is to drive different animation systems using VisageLipSync. 
	*
	@param fbaps the FBAPs structure with current interpolated visemes. The interpolated visemes are stored in FBAPs->FAPs->FAP1 
	* and defined with two visemes and blending factor that determines the blending between the two. The visemes and blending factor 
	* are determined by the coarticulation algorithm that ensures the smooth transition between viseme. These interpolated visemes
	* can be directly used for animation for example by mapping viseme onto morph targets 
	* @param currentTime the time in milliseconds, measured from the beginning of the audio signal that is used in VisageLipSync
	* @param lastViseme the last detected viseme 
	* @param audioBuf buffer with current frame of raw audio data
	*/
	void getCurrentData(FBAPs *fbaps, long *currentTime, int *lastViseme, short *audioBuf);

	int isSpeaking();

protected:
	void interpolateVisemes(long timeMs);
	void init();
	void constructorInit();
	
	
// Attributes
public:	
	
	bool sequential;
	int stop_flag;

	int bRealTime;
	int bPreProcess;
	int bAudioFile;
	int bFbaFile;
	int	bEventHandler;
	int bNumberOfFrames;
	int bRecordOver;
	int bPlayOver;

	float fileTime;
	int	  fileSamples;
	int	  nVisemes;

	int* Visemes;
	char* fbaFileName;
//	char* aFileName;
	char* strFileName;

	LipSyncEventHandler* eventH[100];
	CVisemes* cvis;


	//NEW
	int active;
	int currentViseme;
	FBAPs *fbaps;

	long lastTimeMs;
	int lastViseme;
	double interpolationDir;
	long globalStartTime;

	//
	long localTime;

	long timeVisemeOccurs; //A.C. added
	int lastFrameViseme; //A.C. added
		
	int h_heMode; //added by G.	

private:	
	int h_syncMode;
};

}

#endif // __LipSync_h__


